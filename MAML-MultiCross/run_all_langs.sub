#!/bin/bash -l

#SBATCH --job-name=alllangs # Job name
#SBATCH --partition=gpu # Partition
#SBATCH --nodes=1 # Number of nodes
#SBATCH --gres=gpu:1 # Number of GPUs
#SBATCH --ntasks-per-node=1  # Number of tasks
#SBATCH --output=%j.out # Stdout (%j=jobId)
#SBATCH --error=%j.err # Stderr (%j=jobId)
#SBATCH --time=23:00:00 # Walltime
#SBATCH --mem-per-gpu=180000
#SBATCH -A p118
#SBATCH --array=1-55%10
# Load any necessary modules, in this case OpenMPI with CUDA
module purge
module load Python/3.6.8-GCCcore-8.2.0
module load PyTorch/1.9.1-fosscuda-2020b
module load SciPy-bundle/2020.11-fosscuda-2020b

nvcc --version

echo "----------------------------------"

nvidia-smi

python train_maml_system.py $(head -n $SLURM_ARRAY_TASK_ID experiments_langs.txt | tail -n 1) --continue_from_epoch latest --experiment_name all_langs-threewayprotomaml/$SLURM_ARRAY_TASK_ID/
