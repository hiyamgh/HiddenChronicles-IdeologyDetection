#!/bin/bash -l

#SBATCH --job-name=FTDpAR # Job name
#SBATCH --partition=gpu # Partition
#SBATCH --nodes=1 # Number of nodes
#SBATCH --gres=gpu:1 # Number of GPUs
#SBATCH --ntasks-per-node=1  # Number of tasks
#SBATCH --output=finetuneBERTDiscpAR.%j.out # Stdout (%j=jobId)
#SBATCH --error=finetuneBERTDiscpAR.%j.err # Stderr (%j=jobId)
#SBATCH --time=18:00:00 # Walltime
#SBATCH -A p118
#SBATCH --mail-type=ALL
#SBATCH --mail-user=hkg02@mail.aub.edu
#SBATCH --array=1-12%4

# Load any necessary modules, in this case OpenMPI with CUDA
module purge
module load Python/3.6.8-GCCcore-8.2.0
module load PyTorch/1.9.1-fosscuda-2020b
module load SciPy-bundle/2020.11-fosscuda-2020b
# Need to load the Java module for AraBERT
module load Java/11.0.2


nvcc --version

echo "----------------------------------"

nvidia-smi

df_train=input/Discourse_Profiling/df_train_ar.xlsx
df_dev=input/Discourse_Profiling/df_val_ar.xlsx
df_test=sentences/csv/sentences_ocr_corrected_discourse_profiling_ar.xlsx

shared="--train_set $df_train  --dev_set $df_dev --test_set $df_test --output_dir output/discourse_profiling_FT_AR/$SLURM_ARRAY_TASK_ID/"
echo "$(head -n $SLURM_ARRAY_TASK_ID arglist_finetune_bert_dp_ar.txt | tail -n 1)"
python run_bert_discpar.py $(head -n $SLURM_ARRAY_TASK_ID arglist_finetune_bert_dp_ar.txt | tail -n 1)  $shared
