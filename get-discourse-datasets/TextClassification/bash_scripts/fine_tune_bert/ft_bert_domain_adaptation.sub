#!/bin/bash -l

#SBATCH --job-name=ft-da # Job name
#SBATCH --partition=gpu # Partition
#SBATCH --nodes=1 # Number of nodes
#SBATCH --gres=gpu:1 # Number of GPUs
#SBATCH --ntasks-per-node=1  # Number of tasks
#SBATCH --output=job.%j.out # Stdout (%j=jobId)
#SBATCH --error=job.%j.err # Stderr (%j=jobId)
#SBATCH --time=20:00:00 # Walltime
#SBATCH -A p118
#SBATCH --mail-type=ALL
#SBATCH --mail-user=hkg02@mail.aub.edu
#SBATCH --array=1-5%1

# Load any necessary modules, in this case OpenMPI with CUDA
module purge
module load Python/3.6.8-GCCcore-8.2.0
module load PyTorch/1.9.1-fosscuda-2020b
module load SciPy-bundle/2020.11-fosscuda-2020b


nvcc --version

echo "----------------------------------"

nvidia-smi

train_set=$(head -n $SLURM_ARRAY_TASK_ID training_files_da.txt | tail -n 1)
test_set=data/1987.txt
firstjob=1
echo "SLURM ARRAY TASK ID: $SLURM_ARRAY_TASK_ID"

if [ $SLURM_ARRAY_TASK_ID -eq $firstjob ];
    then
        echo "first job"
        python fine_tuning_domain_adaptation.py --train_data_file $train_set --eval_data_file $test_set
    else
        echo "not first job"
        python fine_tuning_domain_adaptation.py --train_data_file $train_set --eval_data_file $test_set --should_continue
fi