#!/bin/bash -l

#SBATCH --job-name=ARG-large # Job name
#SBATCH --partition=gpu # Partition
#SBATCH --nodes=4 # Number of nodes
#SBATCH --ntasks-per-node=1  # Number of tasks
#SBATCH --gres=gpu:1 # Number of GPUs
#SBATCH --output=%j.out # Stdout (%j=jobId)
#SBATCH --time=1:00:00 # Walltime
#SBATCH --mem=125000
#SBATCH -A p118
#SBATCH --array=1-60%1

# Load any necessary modules, in this case OpenMPI with CUDA
module purge
module load PyTorch/1.7.1-fosscuda-2020b
module load SciPy-bundle/2020.11-fosscuda-2020b
module load Java/11.0.2

nvcc --version

echo "----------------------------------"

nvidia-smi

export MASTER_PORT=$(expr 10000 + $(echo -n $SLURM_JOBID | tail -c 4))
export WORLD_SIZE=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE))
echo "WORLD_SIZE="$WORLD_SIZE

master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_ADDR=$master_addr
echo "MASTER_ADDR="$MASTER_ADDR

srun python xmaml.py $(head -n $SLURM_ARRAY_TASK_ID cross_domain_ar_tuning/argumentation_cross_domain_large.txt | tail -n 1)

